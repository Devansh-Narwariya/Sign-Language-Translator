{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdnVa7O8JyjSpj/UyvO40r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devansh-Narwariya/Sign-Language-Translator/blob/main/model_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "wvQgKiOuqU73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M8XdHPbpPzj"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = # Dataset Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rescale the image values so that they lie in between 0 and 1\n",
        "# Reasons:\n",
        "# 1. Normalization:  Rescaling images so that their pixel values lie between 0 and 1 is a form of normalization. \n",
        "# Normalization can help to reduce the impact of differences in the scale of pixel values between images. \n",
        "# This can make it easier for the neural network to learn the underlying patterns in the images, and it can help to improve the performance of the model.\n",
        "# 2. Gradient Descent: Most neural networks use some variant of gradient descent to optimize the model parameters during training. \n",
        "# Gradient descent algorithms work best when the input data is normalized, so that the gradients are consistent across the input dimensions. \n",
        "# Rescaling the pixel values to lie between 0 and 1 can help to ensure that the gradients are consistent and the optimization algorithm converges more quickly.\n",
        "# Avoiding Saturation: If the pixel values in the input images are too large, they can cause the activation functions in the neural network to saturate. \n",
        "# This means that the gradient becomes very small and the optimization algorithm struggles to make progress. \n",
        "# Rescaling the pixel values to lie between 0 and 1 can help to avoid this problem by ensuring that the input to the activation functions is within a suitable range.\n",
        "\n",
        "train_images=train_images/255.\n",
        "test_images=test_images/255."
      ],
      "metadata": {
        "id": "3zdv9_Jupqmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D"
      ],
      "metadata": {
        "id": "RuYLr4bjpxTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Sequential convolutional neural network model\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16,kernel_size=3,padding='SAME',strides=2,activation='relu',input_shape=(256,256,1)),\n",
        "    MaxPooling2D(pool_size=3),\n",
        "    Flatten(),\n",
        "    Dense(26,activation='softmax')\n",
        "])\n",
        "\n",
        "'''\n",
        "Input Layer: The input layer specifies the shape of the input data. \n",
        "In this case, the input data is a 256x256 grayscale image (i.e., one color channel) that is fed into the neural network.\n",
        "\n",
        "Convolutional Layer: The first layer is a convolutional layer with 16 filters, each with a kernel size of 3x3. \n",
        "The layer uses 'SAME' padding, which means that the output feature maps have the same spatial dimensions as the input feature maps. \n",
        "The layer also uses a stride of 2, which means that the output feature maps have half the spatial resolution of the input feature maps. \n",
        "The activation function used is ReLU, which helps to introduce nonlinearity into the network and improve its ability to learn complex patterns in the data.\n",
        "\n",
        "Pooling Layer: The pooling layer uses a max pooling operation with a pool size of 3x3. \n",
        "This reduces the spatial dimensions of the output feature maps by a factor of 3, which helps to reduce the number of parameters in the network and prevent overfitting.\n",
        "\n",
        "Flatten Layer: The flatten layer is used to convert the output of the pooling layer into a 1D vector, which can be fed into a fully connected layer.\n",
        "\n",
        "Dense Layer: The dense layer has 26 units and uses the softmax activation function. \n",
        "This layer is responsible for producing the final output of the neural network, which is a probability distribution over the 26 possible classes. \n",
        "The softmax activation function ensures that the output probabilities sum to 1, which makes it easier to interpret the output as a probability distribution.\n",
        "'''"
      ],
      "metadata": {
        "id": "Sir9PTZxqTKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model optimizer, loss function and metrics\n",
        "\n",
        "opt=tf.keras.optimizers.Adam(learning_rate=0.005)\n",
        "model.compile(optimizer=opt,\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=[])\n",
        "\n",
        "'''\n",
        "Adam Optimizer: Adam is a popular optimization algorithm that is commonly used for training neural networks. \n",
        "It adapts the learning rate based on the gradient of the loss function, which helps to improve the convergence speed and stability of the training process.\n",
        "\n",
        "Learning Rate: The learning rate is a hyperparameter that controls the step size taken during gradient descent optimization. \n",
        "A larger learning rate can lead to faster convergence, but can also cause the algorithm to overshoot the optimal parameters and fail to converge. \n",
        "A smaller learning rate can lead to slower convergence, but can be more stable and robust to noise in the data.\n",
        "\n",
        "Loss Function: The loss function is used to measure how well the model is performing during training. \n",
        "The sparse categorical crossentropy loss function is commonly used for multi-class classification problems, where each instance can only belong to one class. \n",
        "It calculates the difference between the predicted probability distribution and the true label distribution, and is optimized to minimize this difference.\n",
        "\n",
        "Metrics: The metrics parameter can be used to track additional metrics during training, such as accuracy, precision, recall, or F1 score. \n",
        "In this case, no additional metrics are being tracked during training.\n",
        "'''"
      ],
      "metadata": {
        "id": "-_l5uJZOrOoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the labels\n",
        "\n",
        "labels = [\n",
        "    'A',\n",
        "    'B',\n",
        "    'C',\n",
        "    'D',\n",
        "    'E',\n",
        "    'F',\n",
        "    'G',\n",
        "    'H',\n",
        "    'I',\n",
        "    'J',\n",
        "    'K',\n",
        "    'L',\n",
        "    'M',\n",
        "    'N',\n",
        "    'O',\n",
        "    'P',\n",
        "    'Q',\n",
        "    'R',\n",
        "    'S',\n",
        "    'T',\n",
        "    'U',\n",
        "    'V',\n",
        "    'W',\n",
        "    'X',\n",
        "    'Y',\n",
        "    'Z'\n",
        "]"
      ],
      "metadata": {
        "id": "QzlhOhLPrT2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "\n",
        "history=model.fit(train_images[...,np.newaxis],train_labels,epochs=10,batch_size=256)"
      ],
      "metadata": {
        "id": "EhuajV6VruTq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}